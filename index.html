<!DOCTYPE html>
<html>
<head>
    <title>Cheng Shi - AI Researcher</title>

    <!-- Meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

    <!-- Custom Styles -->
    <style>
          :root {
            --primary-color: #4a5568;
            --secondary-color: #718096;
            --accent-color: #2d3748;
            --bg-dark: #f7fafc;
            --bg-card: #ffffff;
            --text-primary: #2d3748;
            --text-secondary: #4a5568;
            --gradient-1: linear-gradient(135deg, #e2e8f0 0%, #cbd5e0 100%);
            --gradient-2: linear-gradient(135deg, #edf2f7 0%, #e2e8f0 100%);
            --gradient-3: linear-gradient(135deg, #718096 0%, #4a5568 100%);
          }
          
          * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
          }
          
          body {
            font-family: 'Inter', sans-serif;
            font-size: 16px;
            background: var(--bg-dark);
            color: var(--text-primary);
            line-height: 1.6;
            overflow-x: hidden;
          }
          
          /* Animated background */
          body::before {
            content: '';
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: 
              radial-gradient(circle at 20% 80%, rgba(113, 128, 150, 0.1) 0%, transparent 50%),
              radial-gradient(circle at 80% 20%, rgba(74, 85, 104, 0.1) 0%, transparent 50%),
              radial-gradient(circle at 40% 40%, rgba(203, 213, 224, 0.1) 0%, transparent 50%);
            z-index: -1;
            animation: float 20s ease-in-out infinite;
          }
          
          @keyframes float {
            0%, 100% { transform: translateY(0px) rotate(0deg); }
            33% { transform: translateY(-20px) rotate(1deg); }
            66% { transform: translateY(10px) rotate(-1deg); }
          }
          
          /* Text animation */
          .animate-text {
            opacity: 0;
            animation: fadeInUp 0.8s ease forwards;
          }
          
          .animate-text:nth-child(1) { animation-delay: 0.1s; }
          .animate-text:nth-child(2) { animation-delay: 0.2s; }
          .animate-text:nth-child(3) { animation-delay: 0.3s; }
          .animate-text:nth-child(4) { animation-delay: 0.4s; }
          .animate-text:nth-child(5) { animation-delay: 0.5s; }
          .animate-text:nth-child(6) { animation-delay: 0.6s; }
          .animate-text:nth-child(7) { animation-delay: 0.7s; }
          .animate-text:nth-child(8) { animation-delay: 0.8s; }
          .animate-text:nth-child(9) { animation-delay: 0.9s; }
          .animate-text:nth-child(10) { animation-delay: 1.0s; }
          
          @keyframes fadeInUp {
            from {
              opacity: 0;
              transform: translateY(30px);
            }
            to {
              opacity: 1;
              transform: translateY(0);
            }
          }
          
          .stagger-animation > * {
            opacity: 0;
            animation: fadeInUp 0.6s ease forwards;
          }
          
          .stagger-animation > *:nth-child(1) { animation-delay: 0.1s; }
          .stagger-animation > *:nth-child(2) { animation-delay: 0.2s; }
          .stagger-animation > *:nth-child(3) { animation-delay: 0.3s; }
          .stagger-animation > *:nth-child(4) { animation-delay: 0.4s; }
          .stagger-animation > *:nth-child(5) { animation-delay: 0.5s; }
          .stagger-animation > *:nth-child(6) { animation-delay: 0.6s; }
          .stagger-animation > *:nth-child(7) { animation-delay: 0.7s; }
          .stagger-animation > *:nth-child(8) { animation-delay: 0.8s; }
          .stagger-animation > *:nth-child(9) { animation-delay: 0.9s; }
          .stagger-animation > *:nth-child(10) { animation-delay: 1.0s; }
          .stagger-animation > *:nth-child(11) { animation-delay: 1.1s; }
          .stagger-animation > *:nth-child(12) { animation-delay: 1.2s; }
          .stagger-animation > *:nth-child(13) { animation-delay: 1.3s; }
          .stagger-animation > *:nth-child(14) { animation-delay: 1.4s; }
          .stagger-animation > *:nth-child(15) { animation-delay: 1.5s; }
          
          #header {
            background: rgba(255, 255, 255, 0.9);
            backdrop-filter: blur(20px);
            border-bottom: 1px solid rgba(74, 85, 104, 0.1);
            padding: 80px 0;
            position: relative;
          }
          
          #header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: var(--gradient-3);
            opacity: 0.05;
            z-index: -1;
          }
          
          #portrait {
            border: 3px solid var(--primary-color);
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(74, 85, 104, 0.2);
            transition: all 0.3s ease;
            opacity: 0;
            animation: fadeInUp 0.8s ease forwards;
            animation-delay: 0.2s;
          }
          
          #portrait:hover {
            transform: translateY(-10px);
            box-shadow: 0 30px 60px rgba(74, 85, 104, 0.3);
          }
          
          #header-text-name {
            font-size: 48px;
            font-weight: 700;
            background: var(--gradient-3);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 10px;
            opacity: 0;
            animation: fadeInUp 0.8s ease forwards;
            animation-delay: 0.4s;
          }
          
          #header-text-email {
            font-size: 18px;
            color: var(--text-secondary);
            font-family: 'JetBrains Mono', monospace;
            margin-bottom: 20px;
            opacity: 0;
            animation: fadeInUp 0.8s ease forwards;
            animation-delay: 0.6s;
          }
          
          .social-links {
            opacity: 0;
            animation: fadeInUp 0.8s ease forwards;
            animation-delay: 0.8s;
          }
          
          .social-links a {
            display: inline-block;
            margin-right: 15px;
            padding: 10px 20px;
            background: rgba(74, 85, 104, 0.1);
            border: 1px solid rgba(74, 85, 104, 0.2);
            border-radius: 25px;
            color: var(--text-primary);
            text-decoration: none;
            transition: all 0.3s ease;
            backdrop-filter: blur(10px);
          }
          
          .social-links a:hover {
            background: var(--primary-color);
            color: white;
            transform: translateY(-2px);
            box-shadow: 0 10px 20px rgba(74, 85, 104, 0.2);
          }
          
          .section-title {
            font-size: 36px;
            font-weight: 600;
            margin-bottom: 30px;
            position: relative;
            display: inline-block;
            opacity: 0;
            animation: fadeInUp 0.8s ease forwards;
          }
          
          .section-title::after {
            content: '';
            position: absolute;
            bottom: -5px;
            left: 0;
            width: 50px;
            height: 3px;
            background: var(--gradient-3);
            border-radius: 2px;
          }
          
          .bio-text {
            font-size: 18px;
            line-height: 1.8;
            color: var(--text-secondary);
            margin-bottom: 40px;
            opacity: 0;
            animation: fadeInUp 0.8s ease forwards;
            animation-delay: 0.2s;
          }
          
          .news-item {
            background: rgba(255, 255, 255, 0.8);
            border: 1px solid rgba(74, 85, 104, 0.1);
            border-radius: 15px;
            padding: 20px;
            margin-bottom: 15px;
            backdrop-filter: blur(10px);
            transition: all 0.3s ease;
            opacity: 0;
            animation: fadeInUp 0.6s ease forwards;
          }
          
          .news-item:hover {
            transform: translateX(10px);
            border-color: var(--primary-color);
            box-shadow: 0 10px 30px rgba(74, 85, 104, 0.15);
          }
          
          .news-date {
            color: var(--primary-color);
            font-weight: 600;
            font-size: 14px;
          }
          
          .paper-card {
            background: rgba(255, 255, 255, 0.8);
            border: 1px solid rgba(74, 85, 104, 0.1);
            border-radius: 15px;
            padding: 25px;
            margin-bottom: 25px;
            backdrop-filter: blur(10px);
            transition: all 0.3s ease;
            opacity: 0;
            animation: fadeInUp 0.6s ease forwards;
          }
          
          .paper-card:hover {
            transform: translateY(-3px);
            border-color: var(--primary-color);
            box-shadow: 0 15px 30px rgba(74, 85, 104, 0.15);
          }
          
          .paper-title {
            font-weight: 600;
            font-size: 18px;
            margin-bottom: 8px;
            color: var(--text-primary);
          }
          
          .paper-venue {
            color: var(--primary-color);
            font-weight: 600;
            font-size: 14px;
            margin-bottom: 8px;
          }
          
          .paper-authors {
            color: var(--text-secondary);
            font-size: 14px;
            margin-bottom: 15px;
          }
          
          .paper-links a {
            display: inline-block;
            margin-right: 10px;
            padding: 6px 12px;
            background: rgba(74, 85, 104, 0.1);
            border: 1px solid rgba(74, 85, 104, 0.2);
            border-radius: 15px;
            color: var(--text-primary);
            text-decoration: none;
            font-size: 12px;
            transition: all 0.3s ease;
          }
          
          .paper-links a:hover {
            background: var(--primary-color);
            color: white;
            transform: translateY(-2px);
          }
          
          .vspace-top {
            margin-top: 60px;
          }
          
          #footer {
            background: rgba(255, 255, 255, 0.9);
            border-top: 1px solid rgba(74, 85, 104, 0.1);
            padding: 60px 0;
            margin-top: 80px;
          }
          
          /* Scrollbar styling */
          ::-webkit-scrollbar {
            width: 8px;
          }
          
          ::-webkit-scrollbar-track {
            background: var(--bg-dark);
          }
          
          ::-webkit-scrollbar-thumb {
            background: var(--primary-color);
            border-radius: 4px;
          }
          
          ::-webkit-scrollbar-thumb:hover {
            background: var(--secondary-color);
          }
          
          /* Responsive design */
          @media (max-width: 768px) {
            #header-text-name {
              font-size: 36px;
            }
            
            .section-title {
              font-size: 28px;
            }
            
            .paper-card {
              padding: 20px;
            }
          }
    </style>
</head>

<body>
    <div id='header'>
        <div class='container'>
            <div class='row align-items-center'>
                <div class="col-md-3 text-center">
                    <img src='imgs/2025portrait.png' class='img-fluid' id='portrait'>
                </div>

                <div class="col-md-9">
                  <div id='header-text-name'>
                      Cheng Shi (石骋)
                  </div>
                  <div id='header-text-email'>
                        <i class="fas fa-envelope"></i> shicheng2025 (at) connect.hku.hk
                  </div>
                  <div class="social-links">
                    <a href="https://github.com/ChengShiest"><i class="fab fa-github"></i> GitHub</a>
                    <a href="https://scholar.google.com/citations?user=CbCrmEcAAAAJ&hl=en"><i class="fas fa-graduation-cap"></i> Scholar</a>
                    <!-- <a href="docs/Anpei_CV.pdf"><i class="fas fa-file-pdf"></i> CV</a> -->
                  </div>
                </div>
            </div>
        </div>
    </div>

    <div class='container'>
        <div class='row vspace-top'>
            <div class='col-lg-10 offset-lg-1'>
                <h1 class="section-title">Bio</h1>
                <p class="bio-text">
                    I am currently a first-year Ph.D. student at Department of Computer Science, <a href="https://www.hku.hk/" style="color: var(--primary-color);">The University of Hong Kong</a>, where I have the privilege of being supervised by Prof. <a href="https://i.cs.hku.hk/~yzyu/index.html" style="color: var(--primary-color);">Yizhou Yu</a> (ACM/IEEE Fellow). Previously, I obtained my Master's degree from <a href="https://www.shanghaitech.edu.cn/" style="color: var(--primary-color);">ShanghaiTech University</a> in 2024, where I was advised by Prof. <a href="https://faculty.sist.shanghaitech.edu.cn/yangsibei/" style="color: var(--primary-color);">Sibei Yang</a>, and my Bachelor's degree from ShanghaiTech University in 2022.
                    My research interests lie at the intersection of computer vision, natural language processing, and multimodal AI. My current research focuses on open-world visual perception and vision foundation models.
                </p>
                
                <div class='vspace-top'>
                    <h1 class="section-title">Recent News</h1>
                </div>
                
                <div class="stagger-animation">
                    <div class="news-item">
                        <div class="news-date">Sep 2025</div>
                        <div>📄 3 papers accepted by <a href="https://neurips.cc/" style="color: var(--primary-color);">NeurIPS 2025</a></div>
                    </div>

                    <div class="news-item">
                        <div class="news-date">Jul 2025</div>
                        <div>📄 1 paper accepted by <a href="https://iccv.thecvf.com/" style="color: var(--primary-color);">ICCV 2025</a></div>
                    </div>

                    <div class="news-item">
                        <div class="news-date">Feb 2025</div>
                        <div>📄 1 paper accepted by <a href="https://cvpr.thecvf.com/" style="color: var(--primary-color);">CVPR 2025</a></div>
                    </div>
                    
                    <div class="news-item">
                        <div class="news-date">Nov 2024</div>
                        <div>🏆 Awarded National Scholarship</div>
                    </div>

                    <div class="news-item">
                        <div class="news-date">Jul 2024</div>
                        <div>📄 2 papers accepted by <a href="https://eccv2024.ecva.net/" style="color: var(--primary-color);">ECCV 2024</a></div>
                    </div>
                    
                    <div class="news-item">
                        <div class="news-date">Nov 2023</div>
                        <div>🏆 Awarded National Scholarship</div>
                    </div>
                </div>

                <div class='vspace-top'>
                    <h1 class="section-title">Publications</h1>
                </div>

                <p style="color: var(--text-secondary); margin-bottom: 30px; opacity: 0; animation: fadeInUp 0.8s ease forwards; animation-delay: 0.2s;"> * denotes equal contribution and † corresponding author</p>
                
                <div class="stagger-animation">
                    <div class='paper-card'>
                        <div class='paper-title'>
                            Vision Transformer Needs More Than Register
                        </div>
                        <div class='paper-venue'>
                            In submission, 2025
                        </div>
                        <div class='paper-authors'>
                            <b>Cheng Shi</b> and Sibei Yang†
                        </div>
                        <div class="paper-links">
                            <a href="#"><i class="fas fa-file-alt"></i> Paper</a>
                        </div>
                    </div>

                    <div class='paper-card'>
                        <div class='paper-title'>
                            Vision Function Layer in Multimodal LLMs
                        </div>
                        <div class='paper-venue'>
                            NeurIPS 2025
                        </div>
                        <div class='paper-authors'>
                            <b>Cheng Shi</b>, Yizhou Yu and Sibei Yang
                        </div>
                        <div class="paper-links">
                            <a href="#"><i class="fas fa-file-alt"></i> Paper</a>
                        </div>
                    </div>

                    <div class='paper-card'>
                        <div class='paper-title'>
                            Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video
                        </div>
                        <div class='paper-venue'>
                            NeurIPS 2025
                        </div>
                        <div class='paper-authors'>
                            Yulin Zhang, <b>Cheng Shi</b>, Yang Wang and Sibei Yang†
                        </div>
                        <div class="paper-links">
                            <a href="#"><i class="fas fa-file-alt"></i> Paper</a>
                        </div>
                    </div>

                    <div class='paper-card'>
                        <div class='paper-title'>
                            Discovering Compositional Hallucination in LVLMs
                        </div>
                        <div class='paper-venue'>
                            NeurIPS 2025
                        </div>
                        <div class='paper-authors'>
                            Ge Zheng, Jiajin Tang, Jiaye Qian, Hanzhuo Huang, <b>Cheng Shi</b> and Sibei Yang†
                        </div>
                        <div class="paper-links">
                            <a href="#"><i class="fas fa-file-alt"></i> Paper</a>
                        </div>
                    </div>
                    <div class='paper-card'>
                        <div class='paper-title'>
                            Part2Object: Hierarchical Unsupervised 3D Instance Segmentation
                        </div>
                        <div class='paper-venue'>
                            ECCV 2024
                        </div>
                        <div class='paper-authors'>
                            <b>Cheng Shi</b>*, Yuling Zhang*, Bin Yang, Jiajin Tang, Yuexin Ma and Sibei Yang†
                        </div>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/2407.10084"><i class="fas fa-file-alt"></i> Paper</a>
                            <a href="https://github.com/SooLab/Part2Object"><i class="fab fa-github"></i> Code</a>
                        </div>
                    </div>
                    
                    <div class='paper-card'>
                        <div class='paper-title'>
                            Plain-DNet: A Plain Multi-Dataset Object Detector
                        </div>
                        <div class='paper-venue'>
                            ECCV 2024
                        </div>
                        <div class='paper-authors'>
                            <b>Cheng Shi</b>*, Yuchen Zhu* and Sibei Yang†
                        </div>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/2407.10083"><i class="fas fa-file-alt"></i> Paper</a>
                            <a href="https://github.com/SooLab/Plain-Det"><i class="fab fa-github"></i> Code</a>
                        </div>
                    </div>

                    <div class='paper-card'>
                        <div class='paper-title'>
                            Zip-Your-CLIP: CLIP Itself is a Good Object-detector
                        </div>
                        <div class='paper-venue'>
                            ICLR 2024
                        </div>
                        <div class='paper-authors'>
                            <b>Cheng Shi</b> and Sibei Yang†
                        </div>
                        <div class="paper-links">
                            <a href="https://openreview.net/forum?id=4JbrdrHxYy"><i class="fas fa-file-alt"></i> Paper</a>
                            <a href="https://github.com/ChengShiest/Zip-Your-CLIP"><i class="fab fa-github"></i> Code</a>
                        </div>
                    </div>

                    <div class='paper-card'>
                        <div class='paper-title'>
                            Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator
                        </div>
                        <div class='paper-venue'>
                            NeurIPS 2023
                        </div>
                        <div class='paper-authors'>
                            Hanzhuo Huang*, Yufan Feng*, <b>Cheng Shi</b>, Lan Xu, Jingyi Yu, and Sibei Yang†
                        </div>
                        <div class="paper-links">
                            <a href="https://arxiv.org/abs/2309.14494"><i class="fas fa-file-alt"></i> Paper</a>
                            <a href="https://github.com/SooLab/Free-Bloom"><i class="fab fa-github"></i> Code</a>
                        </div>
                    </div>

                    <div class='paper-card'>
                        <div class='paper-title'>
                            LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models
                        </div>
                        <div class='paper-venue'>
                            ICCV 2023
                        </div>
                        <div class='paper-authors'>
                            <b>Cheng Shi</b>, and Sibei Yang†
                        </div>
                        <div class="paper-links">
                            <a href="https://chengshiest.github.io/logo/"><i class="fas fa-globe"></i> Project</a>
                            <a href="https://arxiv.org/abs/2309.01155"><i class="fas fa-file-alt"></i> Paper</a>
                        </div>
                    </div>

                    <div class='paper-card'>
                        <div class='paper-title'>
                            EdaDet: Open-Vocabulary Object Detection Using Early Dense Alignment
                        </div>
                        <div class='paper-venue'>
                            ICCV 2023
                        </div>
                        <div class='paper-authors'>
                            <b>Cheng Shi</b>, and Sibei Yang†
                        </div>
                        <div class="paper-links">
                            <a href="https://chengshiest.github.io/edadet"><i class="fas fa-globe"></i> Project</a>
                            <a href="https://arxiv.org/abs/2309.01151"><i class="fas fa-file-alt"></i> Paper</a>
                        </div>
                    </div>

                    <div class='paper-card'>
                        <div class='paper-title'>
                            Contrastive Grouping with Transformer for Referring Image Segmentation
                        </div>
                        <div class='paper-venue'>
                            CVPR 2023
                        </div>
                        <div class='paper-authors'>
                            Jiajin Tang, Ge Zheng, <b>Cheng Shi</b>, and Sibei Yang†
                        </div>
                        <div class="paper-links">
                            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Contrastive_Grouping_With_Transformer_for_Referring_Image_Segmentation_CVPR_2023_paper.pdf"><i class="fas fa-file-alt"></i> Paper</a>
                            <a href="https://github.com/Toneyaya/CGFormer"><i class="fab fa-github"></i> Code</a>
                        </div>
                    </div>

                    <div class='paper-card'>
                        <div class='paper-title'>
                            DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance
                        </div>
                        <div class='paper-venue'>
                            SIGGRAPH 2023
                        </div>
                        <div class='paper-authors'>
                            Longwen Zhang*, Qiwei Qiu*, Hongyang Lin*, Qixuan Zhang, <b>Cheng Shi</b>, Wei Yang, Ye Shi, Sibei Yang†, Lan Xu†, Jingyi Yu†
                        </div>
                        <div class="paper-links">
                            <a href="https://sites.google.com/view/dreamface"><i class="fas fa-globe"></i> Project</a>
                            <a href="https://arxiv.org/abs/2304.03117"><i class="fas fa-file-alt"></i> Paper</a>
                            <a href="https://www.youtube.com/watch?v=yCuvzgGMvPM"><i class="fab fa-youtube"></i> Video</a>
                        </div>
                    </div>

                    <div class='paper-card'>
                        <div class='paper-title'>
                            Spatial and Visual Perspective-Taking via View Rotation and Relation Reasoning for Embodied Reference Understanding
                        </div>
                        <div class='paper-venue'>
                            ECCV 2022
                        </div>
                        <div class='paper-authors'>
                            <b>Cheng Shi</b>, and Sibei Yang†
                        </div>
                        <div class="paper-links">
                            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960199.pdf"><i class="fas fa-file-alt"></i> Paper</a>
                            <a href="https://github.com/ChengShiest/REP-ERU"><i class="fab fa-github"></i> Code</a>
                        </div>
                    </div>
                </div>

            </div>
        </div>
    </div>

    <div id='footer'>
        <div class='container'>
            <div class='row'>
                <div class='col text-center'>
                    <p style="color: var(--text-secondary);">© 2024 Cheng Shi. Built with passion for AI research.</p>
                </div>
            </div>
        </div>
    </div>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
    
    <!-- Custom JavaScript for enhanced interactions -->
    <script>
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
        
        // Add scroll effect to header
        window.addEventListener('scroll', function() {
            const header = document.getElementById('header');
            if (window.scrollY > 100) {
                header.style.background = 'rgba(255, 255, 255, 0.95)';
            } else {
                header.style.background = 'rgba(255, 255, 255, 0.9)';
            }
        });
        
        // Intersection Observer for scroll animations
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -50px 0px'
        };
        
        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.animationPlayState = 'running';
                }
            });
        }, observerOptions);
        
        // Observe all animated elements
        document.querySelectorAll('.section-title, .bio-text, .stagger-animation').forEach(el => {
            observer.observe(el);
        });
    </script>
</body>

</html>
