<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Cheng Shi, Shi Cheng, 石骋, University of Hong Kong, HKU, Computer Vision, AI"> 
<meta name="description" content="Homepage of Cheng Shi">
<title>Cheng Shi (石骋)</title>
<style type="text/css">
body {
    font-family: Georgia, serif;
    background-color: #ffffff;
    color: #000000;
    margin: 0;
    padding: 0;
}

#layout-content {
    max-width: 900px;
    margin: 25px auto;
    padding: 0 20px;
}

h1 {
    font-size: 28px;
    font-weight: bold;
    margin-bottom: 5px;
    color: #000000;
}

h2 {
    font-size: 20px;
    font-weight: bold;
    margin-top: 30px;
    margin-bottom: 15px;
    color: #000000;
    border-bottom: 1px solid #cccccc;
    padding-bottom: 5px;
}

h3 {
    font-size: 16px;
    font-weight: normal;
    margin: 5px 0;
    color: #555555;
}

p {
    line-height: 1.6;
    margin: 10px 0;
}

a {
    color: #0000EE;
    text-decoration: none;
}

a:hover {
    text-decoration: underline;
}

table {
    border-collapse: collapse;
}

td {
    padding: 5px;
    vertical-align: top;
}

.pub-item {
    margin-bottom: 25px;
}

.pub-img {
    width: 200px;
    padding-right: 20px;
}

.pub-img img {
    width: 100%;
    border: 1px solid #cccccc;
}

.pub-content {
    padding-left: 10px;
}

.pub-title {
    font-weight: bold;
    font-size: 15px;
    margin-bottom: 5px;
}

.pub-venue {
    color: #cc0000;
    font-weight: bold;
    margin-bottom: 5px;
}

.pub-authors {
    margin-bottom: 8px;
    font-size: 14px;
}

.pub-links {
    font-size: 14px;
}

.pub-links a {
    margin-right: 10px;
}

.news-item {
    margin-bottom: 10px;
    line-height: 1.6;
}

.news-date {
    display: inline-block;
    width: 80px;
    font-weight: bold;
}

#footer-text {
    text-align: center;
    margin-top: 40px;
    padding: 20px 0;
    color: #666666;
    font-size: 14px;
}

small {
    font-size: 13px;
    color: #666666;
}
</style>
</head>

<body>
<div id="layout-content">

<table style="width: 100%; margin-bottom: 30px;">
    <tbody>
        <tr>
            <td style="width: 220px;">
                <img src="imgs/2025portrait.png" border="0" width="200" alt="Cheng Shi"/>
            </td>
            <td style="padding-left: 40px;"> 
                <h1>Cheng Shi (石骋)</h1>
                <h3>PhD Student</h3>
                <p>
                    Department of Computer Science<br/>
                    The University of Hong Kong<br/>
                    <br/>
                    Email: shicheng2025 [at] connect [dot] hku [dot] hk<br/>
                </p>
                <p> 
                    <a href="https://github.com/ChengShiest">GitHub</a> | 
                    <a href="https://scholar.google.com/citations?user=CbCrmEcAAAAJ&amp;hl=en">Google Scholar</a>
                </p>
            </td>
        </tr>
    </tbody>
</table>

<h2>Biography</h2>
<p>
I am currently a first-year Ph.D. student at Department of Computer Science, <a href="https://www.hku.hk/">The University of Hong Kong</a>, where I have the privilege of being supervised by Prof. <a href="https://i.cs.hku.hk/~yzyu/index.html">Yizhou Yu</a> (ACM/IEEE Fellow). Previously, I obtained my Master's degree from <a href="https://www.shanghaitech.edu.cn/">ShanghaiTech University</a> in 2024, where I was advised by Prof. <a href="https://faculty.sist.shanghaitech.edu.cn/yangsibei/">Sibei Yang</a>, and my Bachelor's degree from ShanghaiTech University in 2022.
</p>
<p>
My research interests lie at the intersection of computer vision, natural language processing, and multimodal AI. My current research focuses on open-world visual perception and vision foundation models.
</p>

<h2>News</h2>
<div class="news-item">
    <span class="news-date">Sep 2025</span> 3 papers accepted by <a href="https://neurips.cc/">NeurIPS 2025</a>.
</div>
<div class="news-item">
    <span class="news-date">Jul 2025</span> 1 paper accepted by <a href="https://iccv.thecvf.com/">ICCV 2025</a>.
</div>
<div class="news-item">
    <span class="news-date">Feb 2025</span> 1 paper accepted by <a href="https://cvpr.thecvf.com/">CVPR 2025</a>.
</div>
<div class="news-item">
    <span class="news-date">Nov 2024</span> Awarded National Scholarship.
</div>
<div class="news-item">
    <span class="news-date">Jul 2024</span> 2 papers accepted by <a href="https://eccv2024.ecva.net/">ECCV 2024</a>.
</div>
<div class="news-item">
    <span class="news-date">Nov 2023</span> Awarded National Scholarship.
</div>

<h2>Selected Publications</h2>
<p><small>* denotes equal contribution and † corresponding author</small></p>

<table style="width: 100%;">
<tbody>

<tr class="pub-item">
    <td class="pub-img">
    </td>
    <td class="pub-content">
        <div class="pub-title">Vision Transformer Needs More Than Register</div>
        <div class="pub-venue">In submission, 2025</div>
        <div class="pub-authors"><b>Cheng Shi</b> and Sibei Yang†</div>
        <div class="pub-links">
            <a href="#">Paper</a>
        </div>
    </td>
</tr>

<tr><td colspan="2">&nbsp;</td></tr>

<tr class="pub-item">
    <td class="pub-img">
    </td>
    <td class="pub-content">
        <div class="pub-title">Vision Function Layer in Multimodal LLMs</div>
        <div class="pub-venue">NeurIPS 2025</div>
        <div class="pub-authors"><b>Cheng Shi</b>, Yizhou Yu and Sibei Yang</div>
        <div class="pub-links">
            <a href="https://arxiv.org/abs/2509.24791">Paper</a> 
            <a href="https://github.com/ChengShiest/Vision-Function-Layer">Code</a>
        </div>
    </td>
</tr>

<tr><td colspan="2">&nbsp;</td></tr>

<tr class="pub-item">
    <td class="pub-img">
    </td>
    <td class="pub-content">
        <div class="pub-title">Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video</div>
        <div class="pub-venue">NeurIPS 2025</div>
        <div class="pub-authors">Yulin Zhang, <b>Cheng Shi</b>, Yang Wang and Sibei Yang†</div>
        <div class="pub-links">
            <a href="#">Paper</a>
        </div>
    </td>
</tr>

<tr><td colspan="2">&nbsp;</td></tr>

<tr class="pub-item">
    <td class="pub-img">
    </td>
    <td class="pub-content">
        <div class="pub-title">Discovering Compositional Hallucination in LVLMs</div>
        <div class="pub-venue">NeurIPS 2025</div>
        <div class="pub-authors">Ge Zheng, Jiajin Tang, Jiaye Qian, Hanzhuo Huang, <b>Cheng Shi</b> and Sibei Yang†</div>
        <div class="pub-links">
            <a href="#">Paper</a>
        </div>
    </td>
</tr>

<tr><td colspan="2">&nbsp;</td></tr>

<tr class="pub-item">
    <td class="pub-img">
        <img src="part2obj/image.png" alt="Part2Object"/>
    </td>
    <td class="pub-content">
        <div class="pub-title">Part2Object: Hierarchical Unsupervised 3D Instance Segmentation</div>
        <div class="pub-venue">ECCV 2024</div>
        <div class="pub-authors"><b>Cheng Shi</b>*, Yuling Zhang*, Bin Yang, Jiajin Tang, Yuexin Ma and Sibei Yang†</div>
        <div class="pub-links">
            <a href="https://arxiv.org/abs/2407.10084">Paper</a> 
            <a href="https://github.com/SooLab/Part2Object">Code</a>
        </div>
    </td>
</tr>

<tr><td colspan="2">&nbsp;</td></tr>

<tr class="pub-item">
    <td class="pub-img">
        <img src="plain/image.png" alt="Plain-DNet"/>
    </td>
    <td class="pub-content">
        <div class="pub-title">Plain-DNet: A Plain Multi-Dataset Object Detector</div>
        <div class="pub-venue">ECCV 2024</div>
        <div class="pub-authors"><b>Cheng Shi</b>*, Yuchen Zhu* and Sibei Yang†</div>
        <div class="pub-links">
            <a href="https://arxiv.org/abs/2407.10083">Paper</a> 
            <a href="https://github.com/SooLab/Plain-Det">Code</a>
        </div>
    </td>
</tr>

<tr><td colspan="2">&nbsp;</td></tr>

<tr class="pub-item">
    <td class="pub-img">
        <img src="zip/fig1.png" alt="Zip-Your-CLIP"/>
    </td>
    <td class="pub-content">
        <div class="pub-title">Zip-Your-CLIP: CLIP Itself is a Good Object-detector</div>
        <div class="pub-venue">ICLR 2024</div>
        <div class="pub-authors"><b>Cheng Shi</b> and Sibei Yang†</div>
        <div class="pub-links">
            <a href="https://openreview.net/forum?id=4JbrdrHxYy">Paper</a> 
            <a href="https://github.com/ChengShiest/Zip-Your-CLIP">Code</a>
        </div>
    </td>
</tr>

<tr><td colspan="2">&nbsp;</td></tr>

<tr class="pub-item">
    <td class="pub-img">
    </td>
    <td class="pub-content">
        <div class="pub-title">Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator</div>
        <div class="pub-venue">NeurIPS 2023</div>
        <div class="pub-authors">Hanzhuo Huang*, Yufan Feng*, <b>Cheng Shi</b>, Lan Xu, Jingyi Yu, and Sibei Yang†</div>
        <div class="pub-links">
            <a href="https://arxiv.org/abs/2309.14494">Paper</a> 
            <a href="https://github.com/SooLab/Free-Bloom">Code</a>
        </div>
    </td>
</tr>

<tr><td colspan="2">&nbsp;</td></tr>

<tr class="pub-item">
    <td class="pub-img">
    </td>
    <td class="pub-content">
        <div class="pub-title">LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models</div>
        <div class="pub-venue">ICCV 2023</div>
        <div class="pub-authors"><b>Cheng Shi</b>, and Sibei Yang†</div>
        <div class="pub-links">
            <a href="https://chengshiest.github.io/logo/">Project</a> 
            <a href="https://arxiv.org/abs/2309.01155">Paper</a>
        </div>
    </td>
</tr>

<tr><td colspan="2">&nbsp;</td></tr>

<tr class="pub-item">
    <td class="pub-img">
    </td>
    <td class="pub-content">
        <div class="pub-title">EdaDet: Open-Vocabulary Object Detection Using Early Dense Alignment</div>
        <div class="pub-venue">ICCV 2023</div>
        <div class="pub-authors"><b>Cheng Shi</b>, and Sibei Yang†</div>
        <div class="pub-links">
            <a href="https://chengshiest.github.io/edadet">Project</a> 
            <a href="https://arxiv.org/abs/2309.01151">Paper</a>
        </div>
    </td>
</tr>

<tr><td colspan="2">&nbsp;</td></tr>

<tr class="pub-item">
    <td class="pub-img">
    </td>
    <td class="pub-content">
        <div class="pub-title">Contrastive Grouping with Transformer for Referring Image Segmentation</div>
        <div class="pub-venue">CVPR 2023</div>
        <div class="pub-authors">Jiajin Tang, Ge Zheng, <b>Cheng Shi</b>, and Sibei Yang†</div>
        <div class="pub-links">
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Contrastive_Grouping_With_Transformer_for_Referring_Image_Segmentation_CVPR_2023_paper.pdf">Paper</a> 
            <a href="https://github.com/Toneyaya/CGFormer">Code</a>
        </div>
    </td>
</tr>

<tr><td colspan="2">&nbsp;</td></tr>

<tr class="pub-item">
    <td class="pub-img">
    </td>
    <td class="pub-content">
        <div class="pub-title">DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance</div>
        <div class="pub-venue">SIGGRAPH 2023</div>
        <div class="pub-authors">Longwen Zhang*, Qiwei Qiu*, Hongyang Lin*, Qixuan Zhang, <b>Cheng Shi</b>, Wei Yang, Ye Shi, Sibei Yang†, Lan Xu†, Jingyi Yu†</div>
        <div class="pub-links">
            <a href="https://sites.google.com/view/dreamface">Project</a> 
            <a href="https://arxiv.org/abs/2304.03117">Paper</a> 
            <a href="https://www.youtube.com/watch?v=yCuvzgGMvPM">Video</a>
        </div>
    </td>
</tr>

<tr><td colspan="2">&nbsp;</td></tr>

<tr class="pub-item">
    <td class="pub-img">
    </td>
    <td class="pub-content">
        <div class="pub-title">Spatial and Visual Perspective-Taking via View Rotation and Relation Reasoning for Embodied Reference Understanding</div>
        <div class="pub-venue">ECCV 2022</div>
        <div class="pub-authors"><b>Cheng Shi</b>, and Sibei Yang†</div>
        <div class="pub-links">
            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136960199.pdf">Paper</a> 
            <a href="https://github.com/ChengShiest/REP-ERU">Code</a>
        </div>
    </td>
</tr>

</tbody>
</table>

<h2>Visitors</h2>
<div style="text-align: center; margin-top: 20px;">
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=YOUR_MAP_ID&amp;cl=ffffff&amp;w=a"></script>
</div>

<div id="footer-text">
    <p>© 2024 Cheng Shi</p>
</div>

</div>
</body>
</html>
