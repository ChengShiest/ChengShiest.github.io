<!DOCTYPE html>

<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="A novel approach that achieves photo-realistic rendering, fast reconstruction, and compact modeling.">
    <meta name="author" content="Anpei Chen*,
                                Zexiang Xu*,
                                Andreas Geiger,
                                Jingyi Yu,
                                Hao Su">

    <title>LogoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models</title>

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
        <link rel="icon" href="img/tensorf_logo.ico" type="image/x-ico">



</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h1 class="nerf_title_v2">LogoPrompt</h1>
<!--    <h2 class="nerf_title_v2">Tensorial Radiance Fields</h2>-->
    <h1 class="nerf_subheader_v2">Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models</h1>
<!--    <h2>TensoRF: Tensorial Radiance Fields</h2>-->
        <h3 class="nerf_subheader_v2">ICCV 2023</h3>
<!--            <p class="abstract">A compact and efficent scene representation</p>-->
    <hr>
    <p class="authors">
        <a href="https://ChengShiest.github.io"> Cheng Shi</a>,
        <a href="https://faculty.sist.shanghaitech.edu.cn/yangsibei/"> Sibei Yang*</a>

    </p>

    <div class="nerf_equal_v2"><span class="text-span_nerf">*</span><span class="text-span_nerf_star">*</span>Denotes Corresponding Author</div>

    </br></br>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://chengshiest.github.io/logo/">Paper</a>
        <a class="btn btn-primary" href="https://chengshiest.github.io/logo/">Code(Coming Soon)</a>
    </div>
</div>



<div class="container">
    <div class="w-container">
        <h2 class="grey-heading_nerf">Our interesting findings</h2>
<!--    <div class="section">-->
        <div class="columns-5 w-row">
            <img src="img/can_intro.png" style="width:100%; margin-right:0px; margin-top:0px;">
        </div>

    </div>

<!--<hr>-->
    </br></br>
    <div data-anchor="slide1" class="section nerf_section">
        <div class="grey_container w-container">
            <h2 class="grey-heading_nerf">
                Abstract
            </h2>
            <p class="paragraph-3 nerf_text">
                Prompt engineering is a powerful tool used to enhance the performance of pre-trained models on downstreamtasks. For example, providing the prompt “Let's think step
                by step” improved GPT-3's reasoning accuracy to 63% on MutiArith while prompting “a photo of” filled with a class
                name enables CLIP to achieve 80% zero-shot accuracy on ImageNet. While previous research has explored prompt
                learning for the visual modality, analyzing what constitutes a good visual prompt specifically for image recognition is
                limited. In addition, existing visual prompt tuning methods' generalization ability is worse than text-only prompting
                tuning. 
            </p>
            <div class="columns-5 w-row">
                <img src="img/perf.png" style="width:70%; margin-right:0px; margin-top:0px;">
            </div>
            <p class="paragraph-3 nerf_text">
                This paper explores our key insight: synthetic
                text images are good visual prompts for vision-language
                models! To achieve that, we propose our LoGoPrompt,
                which reformulates the classification objective to the
                visual prompt selection and addresses the chicken-and-egg
                challenge of first adding synthetic text images as class-wise
                visual prompts or predicting the class first. Without any
                trainable visual prompt parameters, experimental results
                on 16 datasets demonstrate that our method consistently
                outperforms state-of-the-art methods in few-shot learning,
                base-to-new generalization, and domain generalization
            </p>

            <h2 class="grey-heading_nerf">
                Method
            </h2>
            <p class="paragraph-3 nerf_text">
                Overview of LoGoPrompt, which (a) generates class-wise visual prompts as synthetic images with text class
                names and (b) reformulates the classification objective to visual prompt selection to address the chicken-and-egg challenge
                by (c) the proposed min-max contrastive learning.
            </p>
            <div class="columns-5 w-row">
                <img src="img/method.png" style="width:95%; margin-right:0px; margin-top:0px;">
            </div>
        </div>
    </div>



    </br></br>
    <div class="section">
        <s2>Performance</s2>
        <hr>
        <h2 class="grey-heading_nerf">
                Base-to-new Generalization 
        </h2>
        <p class="paragraph-3 nerf_text">
            The superior performance of LoGoPrompt on both base and new classes shows its strong generalizability.
        </p>
        <div class="columns-5 w-row">
            <img src="img/p2.png" style="width:95%; margin-right:0px; margin-top:0px;">
        </div>

        <h2 class="grey-heading_nerf">
                Few-shot classification
        </h2>
        <p class="paragraph-3 nerf_text">
            LoGoPrompt consistently outperforms compared methods on all the 11 datasets. Following CoOp, ResNet-50 of CLIP is used as the vision backbone.
        </p>
        <div class="columns-5 w-row">
            <img src="img/p3.png" style="width:95%; margin-right:0px; margin-top:0px;">
        </div>



        </br></br></br>
<!--    </div>-->

<!--        </br></br>-->
<!--        <s2>Arxiv</s2>-->
<!--        <hr>-->
<!--        <div>-->
<!--            <div class="list-group">-->
<!--                <a href="https://arxiv.org/abs/2103.15595"-->
<!--                   class="list-group-item">-->
<!--                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">-->
<!--                </a>-->
<!--            </div>-->
<!--        </div>-->

        </br></br>
<!--    <div class="section">-->
        <s2>Bibtex</s2>
        <hr>
        <div class="bibtexsection">
    @InProceedings{Shi_2023_ICCV,
    author = {Shi, Cheng and Yang, Sibei},
    title = {LogoPrompt:Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month = {October},
    year = {2023}
    }
        </div>
<!--    </div>-->
    </div>
    <hr>

    <footer>
        <p>This website is partially borrowed from <a href="https://apchenstu.github.io/TensoRF//">TensoRF</a>.
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

<script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd6c33218.js" type="text/javascript"></script>

<!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->

</body>
</html>
